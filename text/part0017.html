<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>未知</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link rel="stylesheet" type="text/css" href="../stylesheet.css"/>
<link rel="stylesheet" type="text/css" href="../page_styles.css"/>
</head>
  <body class="calibre">
  <h1 class="not-in-toc" id="nav_point_124">第9章　综合应用</h1>

  <p class="zw">目前为止，本书介绍的爬虫技术都是应用于一个定制网站，这样可以帮助我们更加专注于学习特定技巧。而在本章中，我们将分析几个真实网站，来看看我们在本书中学过的这些技巧是如何应用的。首先我们使用Google演示一个真实的搜索表单，然后是依赖JavaScript和API的网站Facebook，接下来是典型的在线商店Gap，最后是拥有地图接口的宝马官网。由于这些都是活跃的网站，因此读者在阅读本书时这些网站存在已经发生变更的风险。不过这样也好，因为本章示例的目的是为了向你展示如何应用前面所学的技术，而不是展示如何抓取任何网站。当你选择运行某个示例时，首先需要检查网站结构在示例编写后是否发生过改变，以及当前该网站的条款与条件是否禁止了爬虫。</p>

  <p class="zw">在本章中，我们将介绍如下主题：</p>

  <ul class="calibre5">
    <li class="calibre6">抓取Google搜索结果网页；</li>

    <li class="calibre6">调研Facebook的API；</li>

    <li class="calibre6">在Gap网站中使用多线程；</li>

    <li class="calibre6">对宝马经销商定位页面进行逆向工程。</li>
  </ul>

  <h2 class="sigil_not_in_toc" id="nav_point_125">9.1　Google搜索引擎</h2>

  <p class="zw">为了了解我们对CSS选择器知识的使用情况，我们将会抓取Google的搜索结果。根据第4章中Alexa的数据，Google是全世界最流行的网站之一，而且非常方便的是，该网站结构简单，易于抓取。</p>

  <blockquote class="calibre11">
    <p class="zw"><img alt="栏目1.jpg" class="calibre12" src="../images/00042.jpeg" width="8%"/>　</p>

    <p class="zw">Google国际化版本可能会根据你的地理位置跳转到指定国家（或地区）的版本。在下述示例中，Google将被设置为罗马尼亚的版本，因此你的结果可能会看起来有些区别。</p>
  </blockquote>

  <p class="zw">图9.1所示为Google搜索主页使用浏览器工具加载查看表单元素时的界面。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0901.tif" class="calibre17" src="../images/00047.png" width="90%"/></p>

  <p class="tu_ti">图9.1</p>

  <p class="zw">可以看到，搜索查询存储在输入参数<code class="calibre4">q</code>当中，然后表单提交到<code class="calibre4">action</code>属性设定的<code class="calibre4">/search</code>路径。我们可以通过将<code class="calibre4">test</code>作为搜索条件提交给表单对其进行测试，此时会跳转到类似<code class="calibre4">https://www.google.ro/?gws_ rd=cr,ssl&amp;ei=TuXYWJXqBsGsswHO8YiQAQ#q=test&amp;*</code>的 URL中。确切的URL取决于你的浏览器和地理位置。此外，如果开启了Google实时，那么搜索结果会使用AJAX执行动态加载，而不再需要提交表单。虽然URL中包含了很多参数，但是只有用于查询的参数<code class="calibre4">q</code>是必需的。</p>

  <p class="zw">当URL为<code class="calibre4">https://www.google.com/search?q=test</code>时，也能产生相同的搜索结果，如图9.2所示。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0902.tif" class="calibre15" src="../images/00052.png" width="60%"/></p>

  <p class="tu_ti">图9.2</p>

  <p class="zw">搜索结果的结构可以使用浏览器工具来查看，如图9.3所示。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0903.tif" class="calibre30" src="../images/00058.png" width="75%"/></p>

  <p class="tu_ti">图9.3</p>

  <p class="zw">从图9.3中可以看出，搜索结果是以链接的形式出现的，并且其父元素是class为<code class="calibre4">"r"</code>的<code class="calibre4">&lt;h3&gt;</code>标签。</p>

  <p class="zw">想要抓取搜索结果，我们可以使用第2章中介绍的CSS选择器。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; from lxml.html import fromstring</strong>
<strong class="calibre1">&gt;&gt;&gt; import requests</strong>
<strong class="calibre1">&gt;&gt;&gt; html = requests.get('https://www.google.com/search?q=test')</strong>
<strong class="calibre1">&gt;&gt;&gt; tree = fromstring(html.content)</strong>
<strong class="calibre1">&gt;&gt;&gt; results = tree.cssselect('h3.r a')</strong>
<strong class="calibre1">&gt;&gt;&gt; results</strong>
<strong class="calibre1">[&lt;Element a at 0x7f3d9affeaf8&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9affe890&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9affe8e8&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9affeaa0&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9b1a9e68&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9b1a9c58&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9b1a9ec0&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9b1a9f18&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9b1a9f70&gt;,</strong>
<strong class="calibre1"> &lt;Element a at 0x7f3d9b1a9fc8&gt;]</strong></code></pre>

  <p class="zw">到目前为止，我们已经下载得到了Google的搜索结果，并且使用<code class="calibre4">lxml</code>抽取出其中的链接。在图9.3中，我们发现链接中的真实网站URL之后还包含了一串附加参数，这些参数将用于跟踪点击。</p>

  <p class="zw">下面是我们在页面中找到的第一个链接。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; link = results[0].get('href')</strong>
<strong class="calibre1">&gt;&gt;&gt; link</strong>
<strong class="calibre1">'/url?q=http://www.speedtest.net/&amp;sa=U&amp;ved=0ahUKEwiCqMHNuvbSAhXD6gTMAA&amp;usg=</strong>
<strong class="calibre1">AFQjCNGXsvN-v4izEgZFzfkIvg'</strong></code></pre>

  <p class="zw">这里我们需要的内容是<code class="calibre4">http://www.speedtest.net/</code>，可以使用<code class="calibre4">urlparse</code>模块从查询字符串中将其解析出来。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; from urllib.parse import parse_qs, urlparse</strong>
<strong class="calibre1">&gt;&gt;&gt; qs = urlparse(link).query</strong>
<strong class="calibre1">&gt;&gt;&gt; parsed_qs = parse_qs(qs)</strong>
<strong class="calibre1">&gt;&gt;&gt; parsed_qs</strong>
<strong class="calibre1">{'q': ['http://www.speedtest.net/'],</strong>
<strong class="calibre1"> 'sa': ['U'],</strong>
<strong class="calibre1"> 'ved': ['0ahUKEwiCqMHNuvbSAhXD6gTMAA'],</strong>
<strong class="calibre1"> 'usg': ['AFQjCNGXsvN-v4izEgZFzfkIvg']}</strong>
<strong class="calibre1">&gt;&gt;&gt; parsed_qs.get('q', [])</strong>
<strong class="calibre1">['http://www.speedtest.net/']</strong></code></pre>

  <p class="zw">该查询字符串解析方法可以用于抽取所有链接。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; links = []</strong>
<strong class="calibre1">&gt;&gt;&gt; for result in results:</strong>
<strong class="calibre1">...     link = result.get('href')</strong>
<strong class="calibre1">...     qs = urlparse(link).query</strong>
<strong class="calibre1">...     links.extend(parse_qs(qs).get('q', []))</strong>
<strong class="calibre1">...</strong>
<strong class="calibre1">&gt;&gt;&gt; links</strong>
<strong class="calibre1">['http://www.speedtest.net/',</strong>
<strong class="calibre1">'test',</strong>
<strong class="calibre1">'https://www.test.com/',</strong>
<strong class="calibre1">'https://ro.wikipedia.org/wiki/Test',</strong>
<strong class="calibre1">'https://en.wikipedia.org/wiki/Test',</strong>
<strong class="calibre1">'https://www.sri.ro/verificati-va-aptitudinile-1',</strong>
<strong class="calibre1">'https://www.sie.ro/AgentiaDeSpionaj/test-inteligenta.html',</strong>
<strong class="calibre1">'http://www.hindustantimes.com/cricket/india-vs-australia-live-cricket-scor</strong>
<strong class="calibre1">e-4th-test-dharamsala-day-3/story-8K124GMEBoiKOgiAaaB5bN.html',</strong>
<strong class="calibre1">'https://sports.ndtv.com/india-vs-australia-2017/live-cricket-score-india-v</strong>
<strong class="calibre1">s-australia-4th-test-day-3-dharamsala-1673771',</strong>
<strong class="calibre1">'http://pearsonpte.com/test-format/']</strong></code></pre>

  <p class="zw">成功了！从Google搜索中得到的链接已经被成功抓取出来了。该示例的完整源码位于本书源码文件的<code class="calibre4">chp9</code>文件夹中，其名为<code class="calibre4">scrape_google.py</code>。</p>

  <p class="zw">抓取Google搜索结果时会碰到的一个难点是，如果你的IP出现可疑行为，比如下载速度过快，则会出现验证码图像，如图9.4所示。</p>

  <p class="zw">我们可以使用第7章中介绍的技术来解决验证码图像这一问题，不过更好的方法是降低下载速度，或者在必须高速下载时使用代理，以避免被Google怀疑。过分请求Google会造成你的IP甚至是一个IP段被封禁，几个小时甚至几天无法访问Google的域名，所以请确保你能够礼貌地使用该网站，不会使你的家庭或办公室中的其他人（包括你自己）被列入黑名单。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0904.tif" class="calibre29" src="../images/00063.png" width="55%"/></p>

  <p class="tu_ti">图9.4</p>

  <h2 class="sigil_not_in_toc" id="nav_point_126">9.2　Facebook</h2>

  <p class="zw">为了演示浏览器和API的使用，我们将会研究Facebook的网站。目前，从月活用户数维度来看，Facebook是世界上最大的社交网络之一，因此其用户数据非常有价值。</p>

  <h3 class="calibre13" id="nav_point_127">9.2.1　网站</h3>

  <p class="zw">图9.5所示为Packt出版社的Facebook页面。</p>

  <p class="zw">当你查看该页的源代码时，可以找到最开始的几篇日志，但是后面的日志只有在浏览器滚动时才会通过AJAX加载。另外，Facebook还提供了一个移动端界面，正如第1章所述，这种形式的界面通常更容易抓取。该页面在移动端的展示形式如图9.6所示。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0905.tif" class="calibre14" src="../images/00073.png" width="70%"/></p>

  <p class="tu_ti">图9.5</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0906.tif" class="calibre17" src="../images/00071.png" width="90%"/></p>

  <p class="tu_ti">图9.6</p>

  <p class="zw">当我们与移动端网站进行交互，并使用浏览器工具查看时，会发现该界面使用了和之前相似的结构来处理AJAX事件，因此该方法无法简化抓取。虽然这些AJAX事件可以被逆向工程，但是不同类型的Facebook页面使用了不同的AJAX调用，而且依据我的过往经验，Facebook经常会变更这些调用的结构，所以抓取这些页面需要持续维护。因此，如第5章所述，除非性能十分重要，否则最好使用浏览器渲染引擎执行JavaScript事件，然后访问生成的HTML页面。</p>

  <p class="zw">下面的代码片段使用Selenium自动化登录Facebook，并跳转到给定页面的URL。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">from selenium import webdriver

def get_driver():
    try:
        return webdriver.PhantomJS()
    except:
        return webdriver.Firefox()

def facebook(username, password, url):
    driver = get_driver()
    driver.get('https://facebook.com')
    driver.find_element_by_id('email').send_keys(username)
    driver.find_element_by_id('pass').send_keys(password)
    driver.find_element_by_id('loginbutton').submit()
    driver.implicitly_wait(30)
    # wait until the search box is available,
    # which means it has successfully logged in
    search = driver.find_element_by_name('q')
    # now logged in so can go to the page of interest
    driver.get(url)
    # add code to scrape data of interest here ...</code></pre>

  <p class="zw">然后，可以调用该函数加载你感兴趣的Facebook页面，并使用合法的Facebook邮箱和密码，抓取生成的HTML页面。</p>

  <h3 class="calibre13" id="nav_point_128">9.2.2　Facebook API</h3>

  <p class="zw">如第1章所述，抓取网站是在其数据没有给出结构化格式时的最末之选。而Facebook确实为绝大多数公共或私有（通过你的用户账号）数据提供了API，因此我们需要在构建加强的浏览器抓取之前，首先检查一下这些API提供的访问是否已经能够满足需求。</p>

  <p class="zw">首先要做的事情是确定通过API哪些数据是可用的。为了解决该问题，我们需要先查阅其API文档。开发者文档的网址为<code class="calibre4">https://developers.facebook.com/docs</code>，在这里给出了所有不同类型的API，包括图谱 API，该API中包含了我们想要的信息。如果你需要构建与Facebook的其他交互（通过API或SDK），可以随时查阅该文档，该文档会定期更新并且易于使用。</p>

  <p class="zw">此外，根据文档链接，我们还可以使用浏览器内的图谱 API探索工具，其地址为<code class="calibre4">https://developers.facebook.com/tools/explorer/</code>。如图9.7所示，探索工具是用来测试查询及其结果的很好的地方。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0907.tif" class="calibre30" src="../images/00065.png" width="75%"/></p>

  <p class="tu_ti">图9.7</p>

  <p class="zw">在这里，我可以搜索API，获取PacktPub的Facebook页面ID。图谱探索工具还可以用来生成访问口令，我们可以用它来定位API。</p>

  <p class="zw">想要在Python中使用图谱API，我们需要使用具有更高级请求的特殊访问口令。幸运的是，有一个名为<code class="calibre4">facebook-sdk</code>（<code class="calibre4">https://facebook-sdk.readthedocs.io</code>）的维护良好的库可以供我们使用。我们只需通过<code class="calibre4">pip</code>安装它即可。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">pip install facebook-sdk</strong></code></pre>

  <p class="zw">下面是使用Facebook的图谱API从Packt出版社页面中抽取数据的代码示例。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">In [1]: from facebook import GraphAPI</strong>

<strong class="calibre1">In [2]: access_token = '....' # insert your actual token here</strong>

<strong class="calibre1">In [3]: graph = GraphAPI(access_token=access_token, version='2.7')</strong>

<strong class="calibre1">In [4]: graph.get_object('PacktPub')</strong>
<strong class="calibre1">Out[4]: {'id': '204603129458', 'name': 'Packt'}</strong></code></pre>

  <p class="zw">我们可以看到和基于浏览器的图谱探索工具相同的结果。我们可以通过传递想要抽取的额外信息，来获得页面中的更多信息。要确定使用哪些信息，我们可以在图谱文档中看到页面中所有可用的字段，文档地址为<code class="calibre4">https://developers.facebook.com/docs/graph-api/reference/page/</code>。使用关键字参数<code class="calibre4">fields</code>，我们可以从API中抽取这些额外可用的字段。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">In [5]: graph.get_object('PacktPub', fields='about,events,feed,picture')</strong>
<strong class="calibre1">Out[5]:</strong>
<strong class="calibre1">{'about': 'Packt provides software learning resources, from eBooks to video</strong>
<strong class="calibre1">courses, to everyone from web developers to data scientists.',</strong>
<strong class="calibre1"> 'feed': {'data': [{'created_time': '2017-03-27T10:30:00+0000',</strong>
<strong class="calibre1"> 'id': '204603129458_10155195603119459',</strong>
<strong class="calibre1"> 'message': "We've teamed up with CBR Online to give you a chance to win 5</strong>
<strong class="calibre1">tech eBooks - enter by March 31! http://bit.ly/2mTvmeA"},</strong>
<strong class="calibre1">...</strong>
<strong class="calibre1"> 'id': '204603129458',</strong>
<strong class="calibre1"> 'picture': {'data': {'is_silhouette': False,</strong>
<strong class="calibre1"> 'url':</strong>
<strong class="calibre1">'https://scontent.xx.fbcdn.net/v/t1.0-1/p50x50/14681705_10154660327349459_7</strong>
<strong class="calibre1">2357248532027065_n.png?oh=d0a26e6c8a00cf7e6ce957ed2065e430&amp;oe=59660265'}}}</strong></code></pre>

  <p class="zw">我们可以看到该响应是格式良好的Python字典，我们可以很容易地进行解析。</p>

  <p class="zw">图谱API还提供了很多访问用户数据的其他调用，其文档可以从Facebook的开发者页面中获取，网址为<code class="calibre4">https://developers.facebook.com/docs/graph-api</code>。根据所需数据的不同，你可能还需要创建一个Facebook开发者应用，从而获得可用时间更长的访问口令。</p>

  <h2 class="sigil_not_in_toc" id="nav_point_129">9.3　Gap</h2>

  <p class="zw">为了演示使用网站地图查看内容，我们将使用Gap的网站。</p>

  <p class="zw">Gap拥有一个结构化良好的网站，通过Sitemap可以帮助网络爬虫定位其最新的内容。如果我们使用第1章中学到的技术调研该网站，则会发现在<code class="calibre4">http://www.gap.com/robots.txt</code>这一网址下的<code class="calibre4">robots.txt</code>文件中包含了网站地图的链接。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">Sitemap: http://www.gap.com/products/sitemap_index.xml</code></pre>

  <p class="zw">下面是链接的<code class="calibre4">Sitemap</code>文件中的内容。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"&gt;
    &lt;sitemap&gt;
        &lt;loc&gt;http://www.gap.com/products/sitemap_1.xml&lt;/loc&gt;
        &lt;lastmod&gt;2017-03-24&lt;/lastmod&gt;
    &lt;/sitemap&gt;
    &lt;sitemap&gt;
        &lt;loc&gt;http://www.gap.com/products/sitemap_2.xml&lt;/loc&gt;
        &lt;lastmod&gt;2017-03-24&lt;/lastmod&gt;
    &lt;/sitemap&gt;
&lt;/sitemapindex&gt;</code></pre>

  <p class="zw">如上所示，Sitemap链接中的内容不仅仅是索引，其中又包含了其他<code class="calibre4">Sitemap</code>文件的链接。这些其他的<code class="calibre4">Sitemap</code>文件中则包含了数千种产品类目的链接，比如<code class="calibre4">http://www.gap.com/products/womens-jogger- pants.jsp</code>，如图9.8所示。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0908.tif" class="calibre30" src="../images/00070.png" width="75%"/></p>

  <p class="tu_ti">图9.8</p>

  <p class="zw">这里有大量需要爬取的内容，因此我们将使用第4章中开发的多线程爬虫。你可能还记得该爬虫支持URL模式以匹配页面。我们同样可以定义一个<code class="calibre4">scraper_callback</code>关键字参数变量，可以让我们解析更多链接。</p>

  <p class="zw">下面是爬取Gap网站中<code class="calibre4">Sitemap</code>链接的示例回调函数。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">from lxml import etree
from threaded_crawler import threaded_crawler

def scrape_callback(url, html):
    if url.endswith('.xml'):
        # Parse the sitemap XML file
        tree = etree.fromstring(html)
        links = [e[0].text for e in tree]
        return links
    else:
        # Add scraping code here
        pass</code></pre>

  <p class="zw">该回调函数首先检查下载到的URL的扩展名。如果扩展名为<code class="calibre4">.xml</code>，则认为下载到的URL是<code class="calibre4">Sitemap</code>文件，然后使用<code class="calibre4">lxml</code>的<code class="calibre4">etree</code>模块解析XML文件并从中抽取链接。否则，认为这是一个类目URL，不过本例中还没有实现抓取类目的功能。现在，我们可以在多线程爬虫中使用该回调函数来爬取<code class="calibre4">gap.com</code>了。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">In [1]: from chp9.gap_scraper_callback import scrape_callback</strong>

<strong class="calibre1">In [2]: from chp4.threaded_crawler import threaded_crawler</strong>

<strong class="calibre1">In [3]: sitemap = 'http://www.gap.com/products/sitemap_index.xml'</strong>

<strong class="calibre1">In [4]: threaded_crawler(sitemap, '[gap.com]*',</strong>
<strong class="calibre1">scraper_callback=scrape_callback)</strong>
<strong class="calibre1">10</strong>
<strong class="calibre1">[&lt;Thread(Thread-517, started daemon 140145732585216)&gt;]</strong>
<strong class="calibre1">Exception in thread Thread-517:</strong>
<strong class="calibre1">Traceback (most recent call last):</strong>
<strong class="calibre1">...</strong>
<strong class="calibre1">File "src/lxml/parser.pxi", line 1843, in lxml.etree._parseMemoryDocument</strong>
<strong class="calibre1">(src/lxml/lxml.etree.c:118282)</strong>
<strong class="calibre1">ValueError: Unicode strings with encoding declaration are not supported.</strong>
<strong class="calibre1">Please use bytes input or XML fragments without declaration.</strong></code></pre>

  <p class="zw">不幸的是，<code class="calibre4">lxml</code>期望加载来自字节或XML片段的内容，而我们存储的是Unicode的响应（因为这样可以让我们使用正则表达式进行解析，并且可以更容易地存储到磁盘中，如第3章和第4章所述）。不过，我们依然可以在本函数中访问该URL。虽然效率不高，但是我们可以再次加载页面；如果我们只对XML页面执行该操作，则可以减少请求的数量，从而不会增加太多加载时间。当然，如果我们使用了缓存的话，也可以提高效率。</p>

  <p class="zw">下面我们将重写回调函数。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">import requests

def scrape_callback(url, html):
    if url.endswith('.xml'):
        # Parse the sitemap XML file
        resp = requests.get(url)
        tree = etree.fromstring(resp.content)
        links = [e[0].text for e in tree]
        return links
    else:
        # Add scraping code here
        pass</code></pre>

  <p class="zw">现在，如果我们再次尝试运行，可以看到执行成功。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">In [4]: threaded_crawler(sitemap, '[gap.com]*',</strong>
<strong class="calibre1">scraper_callback=scrape_callback)</strong>
<strong class="calibre1">10</strong>
<strong class="calibre1">[&lt;Thread(Thread-51, started daemon 139775751223040)&gt;]</strong>
<strong class="calibre1">Downloading: http://www.gap.com/products/sitemap_index.xml</strong>
<strong class="calibre1">Downloading: http://www.gap.com/products/sitemap_2.xml</strong>
<strong class="calibre1">Downloading: http://www.gap.com/products/gap-canada-français-index.jsp</strong>
<strong class="calibre1">Downloading: http://www.gap.co.uk/products/index.jsp</strong>
<strong class="calibre1">Skipping</strong>
<strong class="calibre1">http://www.gap.co.uk/products/low-impact-sport-bras-women-C1077315.jsp due</strong>
<strong class="calibre1">to depth Skipping</strong>
<strong class="calibre1">http://www.gap.co.uk/products/sport-bras-women-C1077300.jsp due to depth</strong>
<strong class="calibre1">Skipping</strong>
<strong class="calibre1">http://www.gap.co.uk/products/long-sleeved-tees-tanks-women-C1077314.jsp</strong>
<strong class="calibre1">due to depth Skipping</strong>
<strong class="calibre1">http://www.gap.co.uk/products/short-sleeved-tees-tanks-women-C1077312.jsp</strong>
<strong class="calibre1">due to depth ...</strong></code></pre>

  <p class="zw">和预期一致，<code class="calibre4">Sitemap</code>文件首先被下载，然后是服装类目。在网络爬虫项目中，你会发现自己可能需要修改及调整代码和类，以适应新的问题。这只是从互联网上抓取内容时诸多令人兴奋的挑战之一。</p>

  <h2 class="sigil_not_in_toc" id="nav_point_130">9.4　宝马</h2>

  <p class="zw">为了研究如何对一个新的网站进行逆向工程，我们将以宝马官方网站作为示例。宝马官方网站中有一个查询本地经销商的搜索工具，其网址为<code class="calibre4">https://www.bmw.de/de/home.html?entryType=dlo</code>，界面如图9.9所示。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0909.tif" class="calibre30" src="../images/00069.png" width="75%"/></p>

  <p class="tu_ti">图9.9</p>

  <p class="zw">该工具将地理位置作为输入参数，然后在地图上显示附近的经销商地点，比如在图9.10中以<code class="calibre4">Berlin</code>作为搜索参数。</p>

  <p class="tu"><img alt="\\fuwuqi6\YDStu\18-0069\0910.tif" class="calibre14" src="../images/00077.png" width="70%"/></p>

  <p class="tu_ti">图9.10</p>

  <p class="zw">使用类似Network选项卡的浏览器开发者工具，我们会发现搜索触发了如下AJAX请求。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">https://c2b-services.bmw.com/c2b-localsearch/services/api/v3/
    clients/BMWDIGITAL_DLO/DE/
        pois?country=DE&amp;category=BM&amp;maxResults=99&amp;language=en&amp;
            lat=52.507537768880056&amp;lng=13.425269635701511</code></pre>

  <p class="zw">这里，<code class="calibre4">maxResults</code>参数被设为99。不过，我们可以使用第1章中介绍的技术增大该参数的值，以便在一次请求中下载所有经销商的地点。下面是将<code class="calibre4">maxResults</code>的值增加到<code class="calibre4">1000</code>时的输出结果。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; import requests</strong>
<strong class="calibre1">&gt;&gt;&gt; url =</strong>
<strong class="calibre1">'https://c2b-services.bmw.com/c2b-localsearch/services/api/v3/clients/BMWDI</strong>
<strong class="calibre1">GITAL_DLO/DE/pois?country=DE&amp;category=BM&amp;maxResults=%d&amp;language=en&amp;</strong>
<strong class="calibre1">lat=52.507537768880056&amp;lng=13.425269635701511'</strong>
<strong class="calibre1">&gt;&gt;&gt; jsonp = requests.get(url % 1000)</strong>
<strong class="calibre1">&gt;&gt;&gt; jsonp.content</strong>
<strong class="calibre1">'callback({"status":{</strong>
<strong class="calibre1">...</strong>
<strong class="calibre1">})'</strong></code></pre>

  <p class="zw">AJAX请求提供了<strong class="calibre1">JSONP</strong>格式的数据，其中JSONP是指填充模式的<strong class="calibre1">JSON</strong>（<strong class="calibre1">JSON with padding</strong>）。这里的填充通常是指要调用的函数，而函数的参数则为纯JSON数据，在本例中调用的是<code class="calibre4">callback</code>函数。由于解析库不容易理解这种填充，因此我们需要移除它，使解析数据更合适。</p>

  <p class="zw">要想使用Python的<code class="calibre4">json</code>模块解析该数据，首先需要将填充部分截取掉，我们可以通过切片操作来实现。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; import json</strong>
<strong class="calibre1">&gt;&gt;&gt; pure_json = jsonp.text[jsonp.text.index('(') + 1 :</strong>
<strong class="calibre1">jsonp.text.rindex(')')]</strong>
<strong class="calibre1">&gt;&gt;&gt; dealers = json.loads(pure_json)</strong>
<strong class="calibre1">&gt;&gt;&gt; dealers.keys()</strong>
<strong class="calibre1">dict_keys(['status', 'translation', 'metadata', 'data', 'count'])</strong>
<strong class="calibre1">&gt;&gt;&gt; dealers['count']</strong>
<strong class="calibre1">715</strong></code></pre>

  <p class="zw">现在，我们已经将德国所有的宝马经销商加载到JSON对象中，可以看出目前总共有715个经销商。下面是第一个经销商的数据。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4"><strong class="calibre1">&gt;&gt;&gt; dealers['data']['pois'][0]</strong>
<strong class="calibre1">{'attributes': {'businessTypeCodes': ['NO', 'PR'],</strong>
<strong class="calibre1"> 'distributionBranches': ['T', 'F', 'G'],</strong>
<strong class="calibre1"> 'distributionCode': 'NL',</strong>
<strong class="calibre1"> 'distributionPartnerId': '00081',</strong>
<strong class="calibre1"> 'facebookPlace': '',</strong>
<strong class="calibre1"> 'fax': '+49 (30) 200992110',</strong>
<strong class="calibre1"> 'homepage': 'http://bmw-partner.bmw.de/niederlassung-berlin-weissensee',</strong>
<strong class="calibre1"> 'mail': 'nl.berlin@bmw.de',</strong>
<strong class="calibre1"> 'outletId': '3',</strong>
<strong class="calibre1"> 'outletTypes': ['FU'],</strong>
<strong class="calibre1"> 'phone': '+49 (30) 200990',</strong>
<strong class="calibre1"> 'requestServices': ['RFO', 'RID', 'TDA'],</strong>
<strong class="calibre1"> 'services': ['EB', 'PHEV']},</strong>
<strong class="calibre1"> 'category': 'BMW',</strong>
<strong class="calibre1"> 'city': 'Berlin',</strong>
<strong class="calibre1"> 'country': 'Germany',</strong>
<strong class="calibre1"> 'countryCode': 'DE',</strong>
<strong class="calibre1"> 'dist': 6.662869863289401,</strong>
<strong class="calibre1"> 'key': '00081_3',</strong>
<strong class="calibre1"> 'lat': 52.562568863415,</strong>
<strong class="calibre1"> 'lng': 13.463589476607,</strong>
<strong class="calibre1"> 'name': 'BMW AG Niederlassung Berlin Filiale Weißensee',</strong>
<strong class="calibre1"> 'oh': None,</strong>
<strong class="calibre1"> 'postalCode': '13088',</strong>
<strong class="calibre1"> 'postbox': None,</strong>
<strong class="calibre1"> 'state': None,</strong>
<strong class="calibre1"> 'street': 'Gehringstr. 20'}</strong></code></pre>

  <p class="zw">现在可以保存我们感兴趣的数据了。下面的代码片段将经销商的名称和经纬度写入一个电子表格当中。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">with open('../../data/bmw.csv', 'w') as fp:
    writer = csv.writer(fp)
    writer.writerow(['Name', 'Latitude', 'Longitude'])
    for dealer in dealers['data']['pois']:
        name = dealer['name']
        lat, lng = dealer['lat'], dealer['lng']
        writer.writerow([name, lat, lng])</code></pre>

  <p class="zw">运行该示例后，得到的<code class="calibre4">bmw.csv</code>表格中的内容类似如下所示。</p>
  <pre class="dai_ma_wu_xing_hao"><code class="calibre4">Name,Latitude,Longitude
BMW AG Niederlassung Berlin Filiale
Weissensee,52.562568863415,13.463589476607
Autohaus Graubaum GmbH,52.4528925,13.521265
Autohaus Reier GmbH &amp; Co. KG,52.56473,13.32521
...</code></pre>

  <p class="zw">从宝马官网抓取数据的完整源代码位于本书源码文件的<code class="calibre4">chp9</code>文件夹中，其名为<code class="calibre4">bmw_scraper.py</code>。</p>

  <blockquote class="calibre11">
    <p class="zw"><img alt="栏目1.jpg" class="calibre12" src="../images/00079.jpeg" width="8%"/>　<strong class="calibre1">翻译外文内容</strong>　</p>

    <p class="zw">你可能已经注意到宝马的第一个截图（见图9.8）是德文的，而第二个截图（见图9.9）是英文的。这是因为第二个截图中的文本使用了Google翻译的浏览器扩展进行了翻译。当尝试了解如何在外文网站中定位时，这是一个非常有用的技术。宝马官网在经过翻译后，仍然可以正常运行。不过还是要当心Google翻译可能会破坏一些网站的正常运行，比如依赖原始值的表单，其中的下拉菜单内容被翻译时就会出现问题。</p>

    <p class="zw">在Chrome中，Google翻译可以通过安装<code class="calibre4">Google Translate</code>扩展获得；在Firefox中，可以安装<code class="calibre4">Google Translator</code>插件；而在IE中，则可以安装<code class="calibre4">Google Toolbar</code>。此外，还可以使用<code class="calibre4">http://translate.google.com</code>进行翻译，不过这样只会对原始文本有用，因此它不会保存格式。</p>
  </blockquote>

  <h2 class="sigil_not_in_toc" id="nav_point_131">9.5　本章小结</h2>

  <p class="zw">本章分析了几个著名网站，并演示了如何在其中应用本书中介绍过的技术。我们在抓取Google结果页时使用了CSS选择器，对Facebook页面测试了浏览器渲染引擎和API，在爬取Gap时使用了<code class="calibre4">Sitemap</code>，在从地图中抓取所有宝马经销商时利用了AJAX调用。</p>

  <p class="zw">现在，你可以运用本书中介绍的技术来抓取包含有你感兴趣数据的网站了。正如本章的演示，本书中所学的工具和方法可以帮助你从互联网上抓取许多不同的网站和内容。我希望这将开启你抽取网络内容以及使用Python进行自动化数据抽取的漫长而又硕果累累的生涯！</p>

  <p class="zw"><br class="sgc-toc-level1"/></p>
  <img alt="图像" class="calibre33" src="../images/00083.jpeg"/>
</body>
</html>
